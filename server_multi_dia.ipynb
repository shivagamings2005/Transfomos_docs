{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCONFIGURATION STEPS\\n- git clone https://github.com/clovaai/CRAFT-pytorch.git\\n- cd CRAFT-pytorch\\n- Download : https://drive.usercontent.google.com/download?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ&export=download&authuser=0\\n- set path in line 26 and 156\\n- open CRAFT-pytorch\\x08asenet\\x0bgg16_bn.py \\n    - remove line 7 and 26\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from base64 import b64encode,b64decode\n",
    "from magic import Magic\n",
    "from cv2 import imencode,imdecode,resize,cvtColor,COLOR_BGR2RGB,IMREAD_COLOR,INTER_AREA\n",
    "from sys import path\n",
    "from torch import device,cuda,no_grad,load,from_numpy\n",
    "from fitz import open\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "from transformers import ViTFeatureExtractor, RobertaTokenizer, TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from docx import Document\n",
    "from pdf2image import convert_from_bytes\n",
    "from docx.document import Document as _Document\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.oxml.table import CT_Tbl\n",
    "from docx.table import _Cell, Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "from pptx import Presentation\n",
    "from hashlib import sha256\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from pytz import UTC\n",
    "from os import path as os_path\n",
    "from pptx.shapes.picture import Picture\n",
    "from flask import Flask, request, jsonify\n",
    "from collections import OrderedDict\n",
    "from warnings import filterwarnings\n",
    "from ocr_tamil.ocr import OCR\n",
    "from lxml import etree\n",
    "import Hack_pdf as hp\n",
    "filterwarnings(\"ignore\")\n",
    "path.append(r\"D:\\sih_check\\CRAFT-pytorch\")\n",
    "from craft import CRAFT\n",
    "from imgproc import resize_aspect_ratio, normalizeMeanVariance\n",
    "from craft_utils import getDetBoxes, adjustResultCoordinates\n",
    "from spacy import load as spacy_load\n",
    "from re import findall\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\"\"\"\n",
    "CONFIGURATION STEPS\n",
    "- git clone https://github.com/clovaai/CRAFT-pytorch.git\n",
    "- cd CRAFT-pytorch\n",
    "- Download : https://drive.usercontent.google.com/download?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ&export=download&authuser=0\n",
    "- set path in line 26 and 156\n",
    "- open CRAFT-pytorch\\basenet\\vgg16_bn.py \n",
    "    - remove line 7 and 26\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): RobertaForCausalLM(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English TrOCR model\n",
    "processor_eng = TrOCRProcessor.from_pretrained('microsoft/trocr-large-stage1')\n",
    "model_eng = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-stage1')\n",
    "\n",
    "model_tam =  OCR(detect=False)\n",
    "\n",
    "feature_extractor_hin=ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "tokenizer_hin = RobertaTokenizer.from_pretrained('flax-community/roberta-hindi')\n",
    "processor_hin = TrOCRProcessor(feature_extractor=feature_extractor_hin, tokenizer=tokenizer_hin)\n",
    "model_hin = VisionEncoderDecoderModel.from_pretrained(r\"D:\\sih\\final_selected\\hindi_handwritten\")\n",
    "\n",
    "device = device(\"cuda\" if cuda.is_available() else \"cpu\") \n",
    "model_eng.to(device)\n",
    "#model_tam.to(device)\n",
    "model_hin.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyStateDict(state_dict):\n",
    "    \"\"\"Copy state dictionary to remove module prefix if present.\"\"\"\n",
    "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
    "        start_prefix = \"module.\"\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[len(start_prefix):]\n",
    "            new_state_dict[name] = v\n",
    "        return new_state_dict\n",
    "    return state_dict\n",
    "\n",
    "def detect_text(image, device,trained_model_path='craft_mlt_25k.pth'):\n",
    "    # Set up device\n",
    "    net = CRAFT()\n",
    "    net.load_state_dict(copyStateDict(load(trained_model_path, map_location=device)))\n",
    "    net.eval()\n",
    "    net = net.to(device)\n",
    "    \n",
    "    # Resize and normalize image\n",
    "    img_resized, target_ratio, _ = resize_aspect_ratio(\n",
    "        image, \n",
    "        square_size=1280, \n",
    "        interpolation=INTER_AREA\n",
    "    )\n",
    "    ratio_h = ratio_w = 1 / target_ratio\n",
    "    \n",
    "    # Prepare image tensor\n",
    "    x = normalizeMeanVariance(img_resized)\n",
    "    x = from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "    x = x.unsqueeze(0)   # [c, h, w] to [b, c, h, w]\n",
    "    x = x.to(device)\n",
    "    \n",
    "    # Inference\n",
    "    with no_grad():\n",
    "        y,_ = net(x)\n",
    "    \n",
    "    # Post-processing\n",
    "    score_text = y[0,:,:,0].cpu().data.numpy()\n",
    "    score_link = y[0,:,:,1].cpu().data.numpy()\n",
    "    \n",
    "    # Get bounding boxes\n",
    "    boxes, _ = getDetBoxes(score_text, score_link, text_threshold=0.62, link_threshold=0.4, low_text=0.2)\n",
    "    # Scale boxes back to original image size\n",
    "    boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "    centers = [(np.mean(box[:, 0]), np.mean(box[:, 1])) for box in boxes]\n",
    "    \n",
    "    # Combine boxes with their centers for easy processing\n",
    "    boxes_with_centers = list(zip(boxes, centers))\n",
    "        \n",
    "    # Sort boxes by Y-axis center for initial clustering\n",
    "    boxes_with_centers.sort(key=lambda b: b[1][1])\n",
    "        \n",
    "    # Efficient row clustering using a sweep line approach\n",
    "    rows = []\n",
    "    current_row = [boxes_with_centers[0]]\n",
    "        \n",
    "    for i in range(1, len(boxes_with_centers)):\n",
    "        _, center = boxes_with_centers[i]\n",
    "        _, prev_center = current_row[-1]\n",
    "            \n",
    "        if abs(center[1] - prev_center[1]) <= 5:\n",
    "            current_row.append(boxes_with_centers[i])\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [boxes_with_centers[i]]\n",
    "        \n",
    "    # Add the last row\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "        \n",
    "    # Sort each row by X-axis center\n",
    "    for row in rows:\n",
    "        row.sort(key=lambda b: b[1][0])\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterApp:\n",
    "    def __init__(self):\n",
    "        self.magic = Magic()\n",
    "    \n",
    "    def detect_language(self, image):\n",
    "        osd_data = pytesseract.image_to_osd(image, output_type=Output.DICT)\n",
    "        script = osd_data.get('script', 'Unknown')\n",
    "        orientation = osd_data.get('orientation', 'Unknown')\n",
    "        return script, orientation\n",
    "    \n",
    "    def process_file(self, file_content):\n",
    "        try:\n",
    "            file_format = self.magic.from_buffer(file_content)\n",
    "            content = {}\n",
    "            print(file_format)\n",
    "\n",
    "            if \"image\" in file_format.lower():\n",
    "                nparr = np.frombuffer(file_content, np.uint8)\n",
    "                image = imdecode(nparr, IMREAD_COLOR)\n",
    "                content = self.process_image(image)\n",
    "            elif \"pdf\" in file_format.lower():\n",
    "                content = self.process_pdf(file_content)\n",
    "            elif \"word\" in file_format.lower():\n",
    "                content = self.process_docx(file_content)\n",
    "            elif \"powerpoint\" in file_format.lower():\n",
    "                content = self.process_pptx(file_content)\n",
    "            else:\n",
    "                print(\"error: Unsupported file type.\")\n",
    "                return -1\n",
    "            print(\"process file\")\n",
    "            return content\n",
    "        except PermissionError:\n",
    "            print(\"Permission denied\")\n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {e}\")\n",
    "            return -1\n",
    "    \n",
    "    def preprocess_image(self,image):\n",
    "        img = cvtColor(image, COLOR_BGR2RGB)  # Convert to RGB\n",
    "        img = resize(img, (160, 80))  # Resize to model input size\n",
    "        img = np.array(img).astype('float32') / 255.0\n",
    "        mean = np.array([0.485, 0.456, 0.406], dtype='float32')\n",
    "        std = np.array([0.229, 0.224, 0.225], dtype='float32')\n",
    "        img = (img - mean) / std\n",
    "        img = np.transpose(img, (2, 0, 1))  # CHW format\n",
    "        img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "        return img\n",
    "\n",
    "    def perform_ocr(self,image):\n",
    "        h,w,_=image.shape\n",
    "        rows=detect_text(image,device)\n",
    "        text=\"\"\n",
    "        # Extract and draw bounding boxes\n",
    "        ordered_words = []\n",
    "        script_to_model = {\n",
    "            'Latin': (model_eng, processor_eng),       # English\n",
    "            'Tamil': \"\",     # Tamil\n",
    "            'Devanagari': (model_hin, processor_hin)   # Hindi\n",
    "        }\n",
    "        ordered_coordinates = list()\n",
    "        for row in rows:\n",
    "            for box, _ in row:\n",
    "                box = np.int32(box)\n",
    "                x_coords = box[:, 0]\n",
    "                y_coords = box[:, 1]\n",
    "                xmin, xmax = int(np.min(x_coords)), int(np.max(x_coords))\n",
    "                ymin, ymax = int(np.min(y_coords)), int(np.max(y_coords))\n",
    "                xmin=max(0,xmin)\n",
    "                xmax=min(w,xmax)\n",
    "                ymin=max(0,ymin)\n",
    "                ymax=min(h,ymax)\n",
    "                ordered_words.append(image[ymin:ymax, xmin:xmax])\n",
    "                ordered_coordinates.append((xmin,ymin,xmax,ymax))\n",
    "        print(f\"[INFO] Detecting the script/language...\")\n",
    "        script, confidendence = self.detect_language(image) #word\n",
    "        if script == \"Fraktur\" or script == \"Malayalam\":\n",
    "            script=\"Tamil\"\n",
    "        elif script == \"Greek\":\n",
    "            script=\"Devanagari\"\n",
    "        print(f\"[INFO] Detected Script: {script}, confidendence: {confidendence}°\")\n",
    "        if script not in script_to_model:\n",
    "            print(\"[WARNING] Script not recognized or not supported. Defaulting to English OCR.\")\n",
    "            model, processor = model_eng, processor_eng\n",
    "        elif script != \"Tamil\":\n",
    "            model, processor = script_to_model[script]\n",
    "            print(f\"[INFO] Using OCR model for script: {script}\")\n",
    "        else:\n",
    "            script=\"Tamil\"\n",
    "            print(f\"[INFO] Using OCR model for script: Tamil\")\n",
    "        print(\"[INFO] Performing OCR...\")\n",
    "        for word in ordered_words: \n",
    "            if script!=\"Tamil\":\n",
    "                pixel_values = processor(images=word, return_tensors=\"pt\").pixel_values.to(device) \n",
    "                generated_ids = model.generate(pixel_values)\n",
    "                text += (processor.batch_decode(generated_ids, skip_special_tokens=True)[0]+\" \")\n",
    "            else:\n",
    "                text+=(\" \".join(model_tam.predict(word))+\" \")\n",
    "            print(\"\\n\",text,\"\\n\")\n",
    "            #except pytesseract.TesseractError:\n",
    "            #    print(\"DPI error!\")\n",
    "        print(\"[INFO] OCR Result:\")\n",
    "        print(text)\n",
    "        return [text,ordered_coordinates]\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        ocr_text=\"\"\n",
    "        try:\n",
    "            ocr_text=self.perform_ocr(image)\n",
    "        except IndexError:\n",
    "            pass\n",
    "        _, buffer = imencode('.png', image)\n",
    "        image_base64 = b64encode(buffer).decode('utf-8')\n",
    "        \n",
    "        print(\"ocr\",ocr_text)\n",
    "        return {\n",
    "            \"page_1\": {\n",
    "                \"text\": \"\",\n",
    "                \"recognized_text\": [ocr_text[0]],\n",
    "                \"images\": [image_base64],\n",
    "                \"tables\": [],\n",
    "                \"bbox\": ocr_text[1]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def check_pdf_metadata(self,doc):\n",
    "        metadata = doc.metadata\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        if \"PowerPoint\" in metadata.get('producer', '') or \"PowerPoint\" in metadata.get('creator', ''):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "      \n",
    "    def process_pdf(self, file):\n",
    "        pdf_file = BytesIO(file)\n",
    "        doc = open(stream=pdf_file, filetype=\"pdf\")\n",
    "        if hp.check_pdf(doc):\n",
    "            return -1\n",
    "        is_powerpoint = self.check_pdf_metadata(doc)\n",
    "        content = {}\n",
    "        # Initialize EasyOCR reader\n",
    "        \n",
    "        if is_powerpoint:\n",
    "            pages = convert_from_bytes(file,poppler_path=r\"C:\\\\poppler-24.07.0\\\\Library\\\\bin\")\n",
    "            for i, page in enumerate(pages):\n",
    "                buffered = BytesIO()\n",
    "                page.save(buffered, format=\"PNG\")\n",
    "                image_base64 = b64encode(buffered.getvalue()).decode('utf-8')\n",
    "                \n",
    "                image_bytes = b64decode(image_base64)\n",
    "                \n",
    "                nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "                img = imdecode(nparr, IMREAD_COLOR)\n",
    "                ocr_text=\"\"\n",
    "                try:\n",
    "                    ocr_text=self.perform_ocr(img)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "                content[f\"page_{i + 1}\"] = {\n",
    "                    \"text\": [],\n",
    "                    \"recognized_text\": [ocr_text[0]],\n",
    "                    \"images\": [image_base64],\n",
    "                    \"tables\": [],\n",
    "                    \"bbox\" : ocr_text[1]\n",
    "                }\n",
    "                \n",
    "        else:\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                text = page.get_text().strip()\n",
    "                image_list = page.get_images(full=True)\n",
    "                images=[]\n",
    "                ocr_text=[]\n",
    "                ocr_bounds=[]\n",
    "                for img in image_list:\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                    image_base64 = b64encode(image_bytes).decode('utf-8')\n",
    "                    rect_obj = page.get_image_bbox(img)\n",
    "                    base_offset = (rect_obj.x0, rect_obj.y0,rect_obj.x1,rect_obj.y1)\n",
    "                    image_np = np.frombuffer(image_bytes, np.uint8)\n",
    "                    cv_img = imdecode(image_np, IMREAD_COLOR) \n",
    "                    images.append(image_base64)\n",
    "                    try:    \n",
    "                        new_text,bounds = self.perform_ocr(cv_img)\n",
    "                        ocr_text.append(new_text+\" \")\n",
    "                        for bound in bounds:\n",
    "                            temp = (base_offset[0] + bound[0], base_offset[1] + bound[1] ,base_offset[2] + bound[2], base_offset[3] + bound[3])\n",
    "                            ocr_bounds.append(temp)\n",
    "                    except IndexError :\n",
    "                        ocr_text.append(\"\")\n",
    "                        continue\n",
    "                content[f\"page_{page_num + 1}\"] = {\n",
    "                    \"text\": text,\n",
    "                    \"recognized_text\": [ocr_text],\n",
    "                    \"images\": images,\n",
    "                    \"tables\": [],\n",
    "                    \"bbox\" : ocr_bounds\n",
    "                }  \n",
    "                print(content,\"\\n\")\n",
    "        doc.close()\n",
    "        return content\n",
    "\n",
    "    def iter_block_items(self,parent):\n",
    "        if isinstance(parent, _Document):\n",
    "            parent_elm = parent.element.body\n",
    "        elif isinstance(parent, _Cell):\n",
    "            parent_elm = parent._tc\n",
    "        else:\n",
    "            raise ValueError(\"something's not right\")\n",
    "\n",
    "        for child in parent_elm.iterchildren():\n",
    "            if isinstance(child, CT_P):\n",
    "                yield Paragraph(child, parent)\n",
    "            elif isinstance(child, CT_Tbl):\n",
    "                yield Table(child, parent)\n",
    "\n",
    "    def extract_table_content(self,table):\n",
    "        table_data = []\n",
    "        for row in table.rows:\n",
    "            row_data = [cell.text.strip() for cell in row.cells]\n",
    "            table_data.append(row_data)\n",
    "        return table_data\n",
    "\n",
    "    def process_docx(self, file):\n",
    "        if hp.check_docx(file):\n",
    "            return -1\n",
    "        docx_file = BytesIO(file)\n",
    "        doc = Document(docx_file)\n",
    "        content = { \n",
    "            \"text\":\"\",\n",
    "            \"recognized_text\": [],\n",
    "            \"images\": [],\n",
    "            \"tables\": []\n",
    "        }\n",
    "        # Extract text and tables\n",
    "        for block in self.iter_block_items(doc):\n",
    "            if isinstance(block, Paragraph):\n",
    "                if(block.text.strip()!=\"\"):\n",
    "                    content[\"text\"]+=(block.text.strip()+\"\\n\")\n",
    "            elif isinstance(block, Table):\n",
    "                content[\"tables\"].append(self.extract_table_content(block))\n",
    "\n",
    "        # Extract images and perform OCR\n",
    "        for rel in doc.part.rels.values():\n",
    "            if \"image\" in rel.reltype:\n",
    "                image_part = rel.target_part\n",
    "                image=image_part.blob\n",
    "                crop_info = getattr(image_part, 'crop', None)\n",
    "                nparr = np.frombuffer(image, np.uint8)\n",
    "                img = imdecode(nparr, IMREAD_COLOR)\n",
    "                if crop_info:\n",
    "                    crop_left = crop_info.get('left', 0)\n",
    "                    crop_top = crop_info.get('top', 0)\n",
    "                    crop_right = crop_info.get('right', 0)\n",
    "                    crop_bottom = crop_info.get('bottom', 0)\n",
    "                    h, w = img.shape[:2]\n",
    "                    crop_left_px = int(w * crop_left)\n",
    "                    crop_top_px = int(h * crop_top)\n",
    "                    crop_right_px = int(w * (1 - crop_right))\n",
    "                    crop_bottom_px = int(h * (1 - crop_bottom))\n",
    "                    img = img[\n",
    "                        crop_top_px:crop_bottom_px, \n",
    "                        crop_left_px:crop_right_px\n",
    "                    ]\n",
    "                ocr_text=\"\"\n",
    "                try:\n",
    "                    ocr_text=self.perform_ocr(img)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                image_base64 = b64encode(image).decode('utf-8')\n",
    "\n",
    "                content[\"images\"].append(image_base64)\n",
    "                content[\"recognized_text\"].append(ocr_text)\n",
    "        \n",
    "        charts = self.extract_charts(doc)\n",
    "        print(charts)\n",
    "        if charts:\n",
    "            content[\"tables\"].extend(charts)\n",
    "\n",
    "        return {\"page_1\":content}\n",
    "    \n",
    "    def extract_charts(self, doc):\n",
    "        \"\"\"\n",
    "        Extract embedded charts and graphs from the docx file and convert them to data tables.\n",
    "        \"\"\"\n",
    "        charts_data = []\n",
    "\n",
    "        for rel in doc.part.rels.values():\n",
    "            if \"chart\" in rel.reltype:\n",
    "                chart_part = rel.target_part\n",
    "                try:\n",
    "                    chart_xml = chart_part.blob\n",
    "                    tree = etree.fromstring(chart_xml)\n",
    "                    namespaces = {\"c\": \"http://schemas.openxmlformats.org/drawingml/2006/chart\"}\n",
    "                    series = tree.xpath(\"//c:ser\", namespaces=namespaces)\n",
    "\n",
    "                    chart_data = {\n",
    "                        \"chart_type\": None,\n",
    "                        \"categories\": [],\n",
    "                        \"series\": {}\n",
    "                    }\n",
    "\n",
    "                    chart_type = tree.xpath(\"//c:chart//c:chartType/text()\", namespaces=namespaces)\n",
    "                    print(chart_type)\n",
    "                    if chart_type:\n",
    "                        chart_data[\"chart_type\"] = chart_type[0]\n",
    "\n",
    "                    for ser in series:\n",
    "                        categories = ser.xpath(\".//c:cat//c:pt//c:v/text()\", namespaces=namespaces)\n",
    "                        values = ser.xpath(\".//c:val//c:pt//c:v/text()\", namespaces=namespaces)\n",
    "                        series_name = ser.xpath(\".//c:tx//c:v/text()\", namespaces=namespaces)\n",
    "\n",
    "                        if series_name:\n",
    "                            series_name = series_name[0]\n",
    "                        else:\n",
    "                            series_name = \"Series\"\n",
    "\n",
    "                        chart_data[\"categories\"].extend(categories)\n",
    "                        chart_data[\"series\"][series_name] = values\n",
    "\n",
    "                    if chart_data[\"categories\"] and chart_data[\"series\"]:\n",
    "                        charts_data.append(chart_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting chart data: {e}\")\n",
    "\n",
    "        return charts_data\n",
    "\n",
    "    def process_pptx(self, file):\n",
    "        pptx_file = BytesIO(file)\n",
    "        presentation = Presentation(pptx_file)\n",
    "        content = {}\n",
    "        tot_pg=0\n",
    "        # Iterate over slides and extract text, images, and tables\n",
    "        for slide in presentation.slides:\n",
    "            tot_pg+=1\n",
    "            text=\"\"\n",
    "            table_data=[]\n",
    "            image_base64=[]\n",
    "            ocr_text=[]\n",
    "            # Extract text from the slide\n",
    "            for shape in slide.shapes:\n",
    "                if shape.has_text_frame:\n",
    "                    for paragraph in shape.text_frame.paragraphs:\n",
    "                        text_content = paragraph.text.strip()\n",
    "                        if text_content:  # Only add non-empty text\n",
    "                            text+=text_content\n",
    "                    text+=\"\\n\"\n",
    "                if shape.has_table:\n",
    "                    table = shape.table\n",
    "                    table_data.append(self.extract_table_content(table))  # Custom method to extract table content\n",
    "                if shape.has_chart:\n",
    "                    chart = shape.chart\n",
    "                    chart_data = {\n",
    "                        \"chart_type\": chart.chart_type,\n",
    "                        \"categories\": [],\n",
    "                        \"series\": {}\n",
    "                    }\n",
    "                    \n",
    "                    # Extract categories (x-axis labels)\n",
    "                    for category in chart.plots[0].categories:\n",
    "                        chart_data[\"categories\"].append(category.label)\n",
    "                    \n",
    "                    # Extract series data\n",
    "                    for series in chart.plots[0].series:\n",
    "                        series_data = [value for value in series.values]\n",
    "                        chart_data[\"series\"][series.name] = series_data\n",
    "                    \n",
    "                    # Store the chart data in the dictionary with slide number as key\n",
    "                    table_data.append(chart_data)\n",
    "        \n",
    "                if isinstance(shape, Picture):\n",
    "                    image = shape.image.blob\n",
    "                    image_np = np.frombuffer(image, np.uint8)  # Convert binary data to NumPy array\n",
    "                    img = imdecode(image_np, IMREAD_COLOR)  # Decode the image to BGR format\n",
    "                    crop_info = {\n",
    "                        'left': shape.crop_left,\n",
    "                        'top': shape.crop_top,\n",
    "                        'right': shape.crop_right,\n",
    "                        'bottom': shape.crop_bottom\n",
    "                    }\n",
    "                    \n",
    "                    # Verify if any crop exists\n",
    "                    if any(crop_info.values()):\n",
    "                        h, w = img.shape[:2]\n",
    "                        \n",
    "                        # Calculate crop pixels\n",
    "                        crop_left_px = int(w * crop_info['left'])\n",
    "                        crop_top_px = int(h * crop_info['top'])\n",
    "                        crop_right_px = int(w * (1 - crop_info['right']))\n",
    "                        crop_bottom_px = int(h * (1 - crop_info['bottom']))\n",
    "                        \n",
    "                        # Crop the image\n",
    "                        img = img[\n",
    "                            crop_top_px:crop_bottom_px, \n",
    "                            crop_left_px:crop_right_px\n",
    "                        ]\n",
    "                    # Perform OCR and base64 encoding\n",
    "                    ocr_text=\"\"\n",
    "                    try:\n",
    "                        ocr_text.append(self.perform_ocr(img))              \n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    image_base64.append(b64encode(image).decode('utf-8'))\n",
    "\n",
    "            content[f\"page_{tot_pg}\"] = {\n",
    "                \"text\": text,\n",
    "                \"recognized_text\": ocr_text,\n",
    "                \"images\": image_base64,\n",
    "                \"tables\": table_data\n",
    "            }\n",
    "        return content\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize text extraction utility with robust error handling\n",
    "        \"\"\"\n",
    "        # Download necessary NLTK resources\n",
    "        try:\n",
    "            download('punkt', quiet=True)\n",
    "            download('stopwords', quiet=True)\n",
    "            download('wordnet', quiet=True)\n",
    "        except:\n",
    "            print(\"Warning: Unable to download all NLTK resources\")\n",
    "        \n",
    "        self.nlp = spacy_load('en_core_web_sm')\n",
    "        \n",
    "        # Initialize lemmatizer and stopwords\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def extract_key_information(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Extract key information with categorized results, avoiding duplicates\n",
    "        \"\"\"\n",
    "        # Process text with SpaCy\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Initialize results dictionary\n",
    "        extracted_info = {\n",
    "            'names': set(),\n",
    "            'locations': set(),\n",
    "            'organizations': set(),\n",
    "            'dates': set(),\n",
    "            'money': set(),\n",
    "            'quantities': set(),\n",
    "            'phone_numbers': set(),\n",
    "            'emails': set(),\n",
    "            'urls': set(),\n",
    "            'other_phrases': set()\n",
    "        }\n",
    "        \n",
    "        # Preserve multi-word named entities\n",
    "        entity_labels = {\n",
    "            'ORG': 'organizations',\n",
    "            'GPE': 'locations',\n",
    "            'LOC': 'locations',\n",
    "            'PERSON': 'names',\n",
    "            'MONEY': 'money',\n",
    "            'QUANTITY': 'quantities'\n",
    "        }\n",
    "        \n",
    "        # Track all extracted values to prevent duplicates\n",
    "        all_extracted_values = set()\n",
    "        \n",
    "        # Extract named entities\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in entity_labels:\n",
    "                category = entity_labels[ent.label_]\n",
    "                value = ent.text.strip()\n",
    "                if value not in all_extracted_values:\n",
    "                    extracted_info[category].add(value)\n",
    "                    all_extracted_values.add(value)\n",
    "        \n",
    "        # Advanced key phrase extraction\n",
    "        def extract_meaningful_phrases(doc):\n",
    "            phrases = []\n",
    "            current_phrase = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in ['NOUN', 'PROPN', 'ADJ']:\n",
    "                    current_phrase.append(token.text)\n",
    "                else:\n",
    "                    if len(current_phrase) > 1:\n",
    "                        phrases.append(' '.join(current_phrase))\n",
    "                    current_phrase = []\n",
    "            \n",
    "            if len(current_phrase) > 1:\n",
    "                phrases.append(' '.join(current_phrase))\n",
    "            \n",
    "            return phrases\n",
    "        \n",
    "        # Extract meaningful multi-word phrases\n",
    "        meaningful_phrases = extract_meaningful_phrases(doc)\n",
    "        for phrase in meaningful_phrases:\n",
    "            if phrase not in all_extracted_values:\n",
    "                extracted_info['other_phrases'].add(phrase)\n",
    "                all_extracted_values.add(phrase)\n",
    "        \n",
    "        # Date extraction\n",
    "        date_patterns = [\n",
    "            r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{4}\\b',  # DD/MM/YYYY or MM/DD/YYYY\n",
    "            r'\\b\\d{4}[/.-]\\d{1,2}[/.-]\\d{1,2}\\b',  # YYYY/MM/DD\n",
    "            r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{4}\\b'  # Month DD, YYYY\n",
    "        ]\n",
    "        \n",
    "        # Extract dates\n",
    "        for pattern in date_patterns:\n",
    "            dates = set(findall(pattern, text))\n",
    "            for date in dates:\n",
    "                if date not in all_extracted_values:\n",
    "                    extracted_info['dates'].add(date)\n",
    "                    all_extracted_values.add(date)\n",
    "        \n",
    "        # Email extraction\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        for email in findall(email_pattern, text):\n",
    "            if email not in all_extracted_values:\n",
    "                extracted_info['emails'].add(email)\n",
    "                all_extracted_values.add(email)\n",
    "        \n",
    "        # Phone number extraction\n",
    "        phone_patterns = [\n",
    "            r'\\b(?:\\+1\\s?)?(?:\\(\\d{3}\\)|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b',  # US/International\n",
    "            r'\\b\\d{10}\\b',  # 10 digit numbers without formatting\n",
    "            r'\\b\\(\\d{3}\\)\\s?\\d{3}[-.]?\\d{4}\\b'  # (XXX) XXX-XXXX format\n",
    "        ]\n",
    "        for pattern in phone_patterns:\n",
    "            found_numbers = findall(pattern, text)\n",
    "            for num in found_numbers:\n",
    "                # Handle both tuple and string results\n",
    "                if isinstance(num, tuple):\n",
    "                    cleaned_num = ''.join(filter(bool, num))\n",
    "                    if cleaned_num and cleaned_num not in all_extracted_values:\n",
    "                        extracted_info['phone_numbers'].add(cleaned_num)\n",
    "                        all_extracted_values.add(cleaned_num)\n",
    "                elif isinstance(num, str):\n",
    "                    if num not in all_extracted_values:\n",
    "                        extracted_info['phone_numbers'].add(num)\n",
    "                        all_extracted_values.add(num)\n",
    "        \n",
    "        # URL extraction\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        for url in findall(url_pattern, text):\n",
    "            if url not in all_extracted_values:\n",
    "                extracted_info['urls'].add(url)\n",
    "                all_extracted_values.add(url)\n",
    "        \n",
    "        # Remove empty sets\n",
    "        extracted_info = {k: v for k, v in extracted_info.items() if v}\n",
    "        \n",
    "        return extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb+srv://jeevajoslin:p7MK68VRoY7LvGXh@tratos.lt7g3.mongodb.net/\")\n",
    "db = client[\"TRANS2\"]  \n",
    "trans_collection = db[\"TRANS\"] \n",
    "userfiles_collection = db[\"USERFILES\"]\n",
    "def generate_sha256(file):\n",
    "    return sha256(file).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npdf2mpdf(input_json, doc): \n",
    "    from fitz import Point \n",
    "    # Iterate through pages in the JSON\n",
    "    for page_index, page_key in enumerate(input_json):\n",
    "        if not page_key.startswith(\"page_\"):\n",
    "            continue\n",
    "\n",
    "        page_index = int(page_key.split(\"_\")[1]) - 1  # Convert to 0-based index\n",
    "        # Get the page from the PDF\n",
    "        page = doc[page_index]\n",
    "\n",
    "        # Get the recognized text and bounding boxes\n",
    "        j_page = input_json[page_key]\n",
    "        recognized_text = [word for line in j_page[\"recognized_text\"] for word in line][0].split()\n",
    "        bounding_boxes = j_page[\"bbox\"]\n",
    "        print(recognized_text)\n",
    "        print(bounding_boxes)\n",
    "        if(len(recognized_text) == len(bounding_boxes)):\n",
    "            print(len(recognized_text))\n",
    "        else:\n",
    "            print(f\"{len(recognized_text)},{len(bounding_boxes)}\")\n",
    "\n",
    "        # Embed each text in its bounding box\n",
    "        for text, bbox in zip(recognized_text, bounding_boxes):\n",
    "            x, y = bbox[0], bbox[1]  # Top-left corner\n",
    "            page.insert_text(\n",
    "                Point(x, y),\n",
    "                text,\n",
    "                fontsize=12,  # Adjust font size as needed\n",
    "                color=(1, 1, 1, 0),  # Fully transparent\n",
    "                render_mode=3  # Invisible text rendering\n",
    "            )\n",
    "\n",
    "    # Save the updated PDF\n",
    "    doc.close()\n",
    "    print(f\"Invisible text embedded successfully.\")\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:6000\n",
      " * Running on http://172.21.7.34:6000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImmutableMultiDict([('file', <FileStorage: 'convert.pdf' (None)>)])\n",
      "abc1\n",
      "{'_id': ObjectId('6759768629befaca3d401a2b'), 'username': 'abc1', 'sha_keys': ['10e4d504dfa3ea4273ee40e3309f798e2f1fa9295478360fc08beb1d6e6abac9', '51a3401687c08430c8fe66d4223a82667999f390e0e9021f730c4b8ace961921', '7697ef618528ba19dd0deb1956f04489cfaf9b2562c0935886c079d1cd8a1186', '6e435d9374dee5005dcb739f4fe9b140f13ed90f54e93cf1497e9d212f593158', 'b9d83d0101344be965a87d3e4ca31844b79b87c61a8f750694968676c2390ac1', '27ca69971f01e5a7adb0ea4f596711804b350739e23670b0a0eab9533f39b1f9', 'c5d2fa2d02a5c56153cae4bb9ab3b85e2b7c1302f085d1edcc10d12da470d956', 'ca33f0ad5af25a7dbfe1776c3e1694cea7b449bac35a0c8801865ce545ce4dce', 'a028abb08649c17d19a095b83226b92a1e78096a32500e6cdff35ffe6b021188', '323fd283c5af5638ec9954080561075e13bb673f942473be754666b74bc49fab', '88a61610c99e37757437f151344fe0cd3640c2e2ab9297e23c3347a21d73d834', '5a9360f11317f8825dd01537df16cbe3a36f615b549ab5ecb42a9ced182c895c', '73f8f221a094f2dd33b73455d1f862c3d433a18ba99e72df58134898721f2fd4', '80bcab9d716fdbe2957fb51f59bf6c6bd934de558080cfac5f9525f69ad4c4f6', '1dc7d4101c75257494d0e56e13239c82cfdde2181f1cc2936665408739fe1ec3', 'dbdf576e52cde21faa38430a0fcfeb75883f672b840ea37a7d2a169ebb039dde', 'f24659ac71bb11d66548a623efd427d8761ca5079190a57373d46e6f04bff841', '07f843e99c291b6663307e9ee9bab8a18ca6a716e6a090382d3955ad85616296', 'b3e746988c6c5098cb11b48ad48f27e504cd1d3c714300ea0ec3133890f0b012', '8ffd7cbc350500021273b108e0efbac5fc5578b50a2919e1ea9cb860586089c1', 'bf9031cf4c625d2dceec6156f224e6309897f76f254dcb8bb5e84cf547993534', 'd8cbbe70e47b633eb0a169e5940bdfa9d4275964bd254f9a6cde859e75cf3c3c', '36359bdcddc5469a55f6c4ed55f54b51460b91dadd8b8ea5521255dedb7f766e', '16ba184455e19a0041f020848bfadc9d4768c0f292d3783efcbc3556b070b919']}\n",
      "65899a6e85951d115c8871dc50be164e52996e9671edf61b6efdada214edb3df\n",
      "PDF document, version 1.7\n",
      "format: PDF 1.7\n",
      "title: \n",
      "author: S S S\n",
      "subject: \n",
      "keywords: \n",
      "creator: Microsoft® Word LTSC\n",
      "producer: Microsoft® Word LTSC\n",
      "creationDate: D:20241212165800+05'30'\n",
      "modDate: D:20241212165800+05'30'\n",
      "trapped: \n",
      "encryption: None\n",
      "[INFO] Detecting the script/language...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.21.7.34 - - [12/Dec/2024 16:59:07] \"POST /student/process HTTP/1.1\" 400 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file: (1, 'Estimating resolution as 264 Too few characters. Skipping this page Warning. Invalid resolution 0 dpi. Using 70 instead. Too few characters. Skipping this page Error during processing.')\n",
      "last <Response 3 bytes [200 OK]>\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# API Routes\n",
    "@app.route('/student/process', methods=['POST'])\n",
    "def upload_file():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({\"error\": \"No file provided\"}), 400\n",
    "    file = request.files['file']\n",
    "    print(request.files)\n",
    "    user = request.form.get('user')\n",
    "    pdf = request.form.get('pdf')  # Retrieve y parameter\n",
    "    pdf = int(pdf) if pdf is not None else None\n",
    "    print(user)\n",
    "    if file.filename == '':\n",
    "        return jsonify({\"error\": \"No selected file\"}), 400\n",
    "    file_content = file.read()\n",
    "\n",
    "    userfiles_document = userfiles_collection.find_one({\"username\":user})\n",
    "    print(userfiles_document)\n",
    "    if not userfiles_document:\n",
    "        return jsonify({\"error\": \"User not found. Cannot upload files.\"}), 404\n",
    "    sha_key = generate_sha256(file_content)\n",
    "    print(sha_key)\n",
    "    existing_document = trans_collection.find_one({\"sha_key\": sha_key})\n",
    "    if existing_document:\n",
    "            # Consistent return type\n",
    "            userfiles_collection.update_one(\n",
    "                {\"username\": user},\n",
    "                {\"$addToSet\": {\"sha_keys\": sha_key}} \n",
    "            )\n",
    "            return jsonify({\n",
    "                \"message\": \"File already exists in the database\",\n",
    "                \"sha_key\": sha_key,\n",
    "                \"existing_content\": existing_document.get(\"json_file_content\", {})\n",
    "            }), 200\n",
    "    content = app.master_app.process_file(file_content)\n",
    "    if content!=-1 and content!=0:\n",
    "        text=\"\"\n",
    "        for _, page_content in content.items():\n",
    "            # Extract recognized text if available\n",
    "            if 'recognised_text' in page_content:\n",
    "                text+=\" \".join(page_content[\"recognised_text\"])\n",
    "            \n",
    "            # Extract regular text if available\n",
    "            if 'text' in page_content:\n",
    "                text+=(\" \"+page_content['text'])\n",
    "        extract=TextExtractor().extract_key_information(text)\n",
    "        meta={}\n",
    "        for category, values in extract.items():\n",
    "            meta[category.replace('_', ' ')]=list(values)\n",
    "        print(\"meta\",meta)\n",
    "        content[\"metadata\"]=meta\n",
    "    print(\"last\",jsonify(content))\n",
    "    \n",
    "    print(content)\n",
    "    if content == -1:\n",
    "        return jsonify({\"error\": \"Unsupported file type\"}), 400\n",
    "    elif content ==0:\n",
    "        return jsonify({\"error\": \"Permission denied\"}), 400\n",
    "    \n",
    "    document = {\n",
    "        \"file_name\": os_path.basename(file.filename),\n",
    "        \"file_type\": os_path.splitext(file.filename)[1],\n",
    "        \"sha_key\": sha_key,\n",
    "        \"processed_at\": datetime.now(UTC)\n",
    "    }\n",
    "    \n",
    "    result = trans_collection.insert_one(document)\n",
    "    trans_collection.update_one(\n",
    "        {\"_id\": result.inserted_id},\n",
    "        {\"$set\": {\"json_file_content\": content}}\n",
    "    )\n",
    "\n",
    "    userfiles_collection.update_one(\n",
    "        {\"username\": user},\n",
    "        {\"$addToSet\": {\"sha_keys\": sha_key}} \n",
    "    )\n",
    "    if pdf==1:\n",
    "        from flask import send_file\n",
    "        pdf_file = BytesIO(file_content)\n",
    "        npdf2mpdf(content,open(stream=pdf_file, filetype=\"pdf\"))\n",
    "        return send_file(\n",
    "            pdf_file, \n",
    "            mimetype='application/pdf', \n",
    "            as_attachment=True, \n",
    "            download_name='modified.pdf'\n",
    "        )\n",
    "    return jsonify(content), 200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.master_app = MasterApp()\n",
    "    app.run(host='0.0.0.0', port=6000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
